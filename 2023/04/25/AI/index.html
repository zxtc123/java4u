<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="Snail">
  <link 
    rel="icon" 
    href="/java4u/img/snail.png">
  <title>人工智能入门</title>
  
    
      <meta 
        property="og:title" 
        content="人工智能入门">
    
    
      <meta 
        property="og:url" 
        content="https://zxtc123.github.io/java4u/2023/04/25/AI/index.html">
    
    
      <meta 
        property="og:img" 
        content="/java4u/img/1.e3a1f41f.gif">
    
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2023-04-25">
      <meta 
        property="og:article:modified_time" 
        content="2023-10-25">
      <meta 
        property="og:article:author" 
        content="Zhao Xin">
      
        
          <meta 
            property="og:article:tag" 
            content="学习">
        
      
    
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
      }
    };
    setDarkmode();
    </script>
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  
    <link rel="preload" href="/java4u/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/java4u/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/java4u/js/lib/lozad.min.js" as="script">
  
  
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-svg.js" as="script">
  
  
  
  
  <link rel="stylesheet" href="/java4u/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
    <link rel="stylesheet" href="/java4u/js/lib/lightbox/baguetteBox.min.css">
  
<meta name="generator" content="Hexo 6.1.0"></head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
        <img 
          class="navbar-logo-img"
          width="32"
          height="32"
          src="/java4u/img/snail.png" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">Snail</span>
    </span>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/java4u/" 
        class="navbar-menu-item">
        
          首页
        
      </a>
    
      <a 
        href="/java4u/archives" 
        class="navbar-menu-item">
        
          归档
        
      </a>
    
      <a 
        href="/java4u/tags" 
        class="navbar-menu-item">
        
          标签
        
      </a>
    
      <a 
        href="/java4u/categories" 
        class="navbar-menu-item">
        
          分类
        
      </a>
    
      <a 
        href="/java4u/about" 
        class="navbar-menu-item">
        
          关于
        
      </a>
    
      <a 
        href="/java4u/links" 
        class="navbar-menu-item">
        
          友链
        
      </a>
    
    <a 
      class="navbar-menu-item darknavbar" 
      id="dark">
      <i class="iconfont icon-weather"></i>
    </a>
    <a 
      class="navbar-menu-item searchnavbar" 
      id="search">
      <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i>
    </a>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      人工智能入门
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2023-04-24T16:00:00.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2023-04-25</span>
    </time>
    
      <span class="dot"></span>
      
        <a 
          href="/java4u/categories/AI/" 
          class="post-meta-link">
          AI
        </a>
      
    
    
      <span class="dot"></span>
      <span>5.8k 字</span>
    
  </div>
  
    <div 
      class="post-meta post-show-meta" 
      style="margin-top: -10px;">
      <div style="display: flex; align-items: center;">
        <i 
          class="iconfont icon-biaoqian" 
          style="margin-right: 2px; font-size: 1.15rem;">
        </i>
        
          
          <a 
            href="/java4u/tags/%E5%AD%A6%E4%B9%A0/" 
            class="post-meta-link">
            学习
          </a>
        
      </div>
    </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <h1 id="人工智能入门"><a href="#人工智能入门" class="headerlink" title="人工智能入门"></a>人工智能入门</h1><h2 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h2><p>机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。列举一些机器学习经典的场景：</p>
<ol>
<li>搜索引擎：根据搜索和使用习惯，优化下一次搜索的结果。</li>
<li>电商网站：自动推荐你可能感兴趣的商品。</li>
<li>贷款申请：通过你最近的金融活动信息进行综合评定。</li>
<li>图像识别：自动识别图片中有没有不和谐的内容。</li>
</ol>
<p>机器学习可以分为监督学习和非监督学习。监督学习是从给定的训练数据集中学习得到一个函数，当新的数据到来时，可以根据这个函数预测结果，监督学习的训练集包括输入和输出，也可以说是特征和目标。监督学习的目标是由人来标注的，而非监督学习的数据没有类别信息，训练集也没有人为标注结果，通过无监督学习可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息 。</p>
<p>实现机器学习的一般步骤：</p>
<ol>
<li>数据收集</li>
<li>数据准备</li>
<li>数据分析</li>
<li>训练算法</li>
<li>测试算法</li>
<li>应用算法</li>
</ol>
<h2 id="一-引言"><a href="#一-引言" class="headerlink" title="一. 引言"></a>一. 引言</h2><h3 id="日常生活机器学习"><a href="#日常生活机器学习" class="headerlink" title="日常生活机器学习"></a>日常生活机器学习</h3><p>通过语音唤醒Siri。收集一个包含大量音频样本的<em>数据集</em>（dataset），定义一个灵活的程序算法，其输出由许多参数决定。 任一调整参数后的程序被称为<em>模型</em>（model）。通过操作参数而生成的所有不同程序（输入-输出映射）的集合称为“模型族”。使用数据集来选择参数的元程序被称为<em>学习算法</em>（learning algorithm）。</p>
<h3 id="1-监督学习"><a href="#1-监督学习" class="headerlink" title="1. 监督学习"></a>1. 监督学习</h3><p>监督学习一般分为3步：</p>
<ol>
<li>从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签（例如，患者是否在下一年内康复？）；有时，这些样本可能需要被人工标记（例如，图像分类）。这些输入和相应的标签一起构成了训练数据集；</li>
<li>选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；</li>
<li>将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。</li>
</ol>
<p><img src="https://zh.d2l.ai/_images/supervised-learning.svg" alt="../_images/supervised-learning.svg" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://zh.d2l.ai/_images/supervised-learning.svg" class="lozad post-image"></p>
<h4 id="1-1-回归"><a href="#1-1-回归" class="headerlink" title="1.1 回归"></a>1.1 回归</h4><p>当标签取任意数值时，我们称之为<em>回归</em>问题，此时的目标是生成一个模型，使它的预测非常接近实际标签值。任何有关“有多少”的问题很可能就是回归问题。比如：</p>
<ul>
<li>这个手术需要多少小时；</li>
<li>在未来6小时，这个镇会有多少降雨量。</li>
</ul>
<h4 id="1-2-分类"><a href="#1-2-分类" class="headerlink" title="1.2 分类"></a>1.2 分类</h4><p><em>分类</em>问题希望模型能够预测样本属于哪个<em>类别</em>（category，正式称为<em>类</em>（class））。 例如，手写数字可能有10类，标签被设置为数字0～9。 最简单的分类问题是只有两类，这被称之为<em>二项分类</em>（binomial classification）。 例如，数据集可能由动物图像组成，标签可能是猫狗{猫,狗}两类。 回归是训练一个回归函数来输出一个数值； 分类是训练一个分类器来输出预测的类别。</p>
<h4 id="1-3-标记问题"><a href="#1-3-标记问题" class="headerlink" title="1.3 标记问题"></a>1.3 标记问题</h4><p>学习预测不相互排斥的类别的问题称为<em>多标签分类</em>（multi-label classification）。举个例子，人们在技术博客上贴的标签，比如“机器学习”“技术”“小工具”“编程语言”“Linux”“云计算”“AWS”。 一篇典型的文章可能会用5～10个标签，因为这些概念是相互关联的。 关于“云计算”的帖子可能会提到“AWS”，而关于“机器学习”的帖子也可能涉及“编程语言”。</p>
<p><img src="https://zh.d2l.ai/_images/stackedanimals.png" alt="../_images/stackedanimals.png" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://zh.d2l.ai/_images/stackedanimals.png" class="lozad post-image"></p>
<h4 id="1-4-搜索"><a href="#1-4-搜索" class="headerlink" title="1.4 搜索"></a>1.4 搜索</h4><p>有时，我们不仅仅希望输出一个类别或一个实值。 在信息检索领域，我们希望对一组项目进行排序。 以网络搜索为例，目标不是简单的“查询（query）-网页（page）”分类，而是在海量搜索结果中找到用户最需要的那部分。</p>
<h4 id="1-5-推荐系统"><a href="#1-5-推荐系统" class="headerlink" title="1.5 推荐系统"></a>1.5 推荐系统</h4><p>它的目标是向特定用户进行“个性化”推荐。 例如，对于电影推荐，科幻迷和喜剧爱好者的推荐结果页面可能会有很大不同。 类似的应用也会出现在零售产品、音乐和新闻推荐等等。</p>
<p><img src="https://zh.d2l.ai/_images/deeplearning-amazon.jpg" alt="../_images/deeplearning-amazon.jpg" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://zh.d2l.ai/_images/deeplearning-amazon.jpg" class="lozad post-image"></p>
<h4 id="1-6-序列学习"><a href="#1-6-序列学习" class="headerlink" title="1.6 序列学习"></a>1.6 序列学习</h4><p>如果输入是连续的，模型可能就需要拥有“记忆”功能。 比如，我们该如何处理视频片段呢？ 在这种情况下，每个视频片段可能由不同数量的帧组成。 通过前一帧的图像，我们可能对后一帧中发生的事情更有把握。 语言也是如此，机器翻译的输入和输出都为文字序列。</p>
<h3 id="2-无监督学习"><a href="#2-无监督学习" class="headerlink" title="2. 无监督学习"></a>2. 无监督学习</h3><p>如果工作没有十分具体的目标，就需要“自发”地去学习了。 比如，老板可能会给我们一大堆数据，然后要求用它做一些数据科学研究，却没有对结果有要求。 这类数据中不含有“目标”的机器学习问题通常被为<em>无监督学习</em>（unsupervised learning）</p>
<h3 id="3-与环境互动"><a href="#3-与环境互动" class="headerlink" title="3. 与环境互动"></a>3. 与环境互动</h3><p>不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为<em>离线学习</em>（offline learning）。这种简单的离线学习有它的魅力。 好的一面是，我们可以孤立地进行模式识别，而不必分心于其他问题。 但缺点是，解决的问题相当有限。 这时我们可能会期望人工智能不仅能够做出预测，而且能够与真实环境互动。 </p>
<h3 id="4-强化学习"><a href="#4-强化学习" class="headerlink" title="4. 强化学习"></a>4. 强化学习</h3><p>在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些<em>观察</em>（observation），并且必须选择一个<em>动作</em>（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得<em>奖励</em>（reward）。 此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。 </p>
<p><img src="https://zh.d2l.ai/_images/rl-environment.svg" alt="../_images/rl-environment.svg" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://zh.d2l.ai/_images/rl-environment.svg" class="lozad post-image"></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul>
<li>机器学习研究计算机系统如何利用经验（通常是数据）来提高特定任务的性能。它结合了统计学、数据挖掘和优化的思想。通常，它是被用作实现人工智能解决方案的一种手段。</li>
<li>表示学习作为机器学习的一类，其研究的重点是如何自动找到合适的数据表示方式。深度学习是通过学习多层次的转换来进行的多层次的表示学习。</li>
<li>深度学习不仅取代了传统机器学习的浅层模型，而且取代了劳动密集型的特征工程。</li>
<li>最近在深度学习方面取得的许多进展，大都是由廉价传感器和互联网规模应用所产生的大量数据，以及（通过GPU）算力的突破来触发的。</li>
<li>整个系统优化是获得高性能的关键环节。有效的深度学习框架的开源使得这一点的设计和实现变得非常容易。</li>
</ul>
<h2 id="二-预备知识"><a href="#二-预备知识" class="headerlink" title="二. 预备知识"></a>二. 预备知识</h2><h3 id="2-1-数据操作"><a href="#2-1-数据操作" class="headerlink" title="2.1 数据操作"></a>2.1 数据操作</h3><pre class="highlight"><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 创建一个行向量x</span></span><br><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">12</span>,), dtype=int32, numpy=array([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>])&gt;</span><br><span class="line"><span class="comment"># 通过shape访问张量的形状</span></span><br><span class="line">x.shape</span><br><span class="line"><span class="comment"># 形状的所有元素乘积</span></span><br><span class="line">tf.size(x)</span><br><span class="line"><span class="comment"># 改变一个张量的形状而不改变元素数量和元素值</span></span><br><span class="line">X = tf.reshape(x, (<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># 通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值</span></span><br><span class="line">tf.constant([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">4</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]], dtype=int32)&gt;</span><br></pre>

<h4 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h4><pre class="highlight"><span class="line">在数学表示法中，我们将通过符号 𝑓:ℝ→ℝ来表示一元标量运算符（只接收一个输入）。 </span><br><span class="line">这意味着该函数从任何实数（ℝ）映射到另一个实数。 同样，我们通过符号 𝑓:ℝ,ℝ→ℝ</span><br><span class="line">表示二元标量运算符，这意味着该函数接收两个输入，并产生一个输出。 </span><br><span class="line">给定同一形状的任意两个向量 𝐮 和 𝐯 和二元运算符 𝑓， 我们可以得到向量 𝐜=𝐹(𝐮,𝐯)。 </span><br><span class="line">具体计算方法是 𝑐𝑖←𝑓(𝑢𝑖,𝑣𝑖)， 其中 𝑐𝑖、 𝑢𝑖 和 𝑣𝑖 分别是向量 𝐜、 𝐮 和 𝐯中的元素。</span><br><span class="line">在这里，我们通过将标量函数升级为按元素向量运算来生成向量值  𝐹:ℝ𝑑,ℝ𝑑→ℝ𝑑。</span><br></pre>

<pre class="highlight"><span class="line">x = tf.constant([1.0, 2, 4, 8])</span><br><span class="line">y = tf.constant([2.0, 2, 2, 2])</span><br><span class="line">x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.,  4.,  6., 10.], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.,  0.,  2.,  6.], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.,  4.,  8., 16.], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5, 1. , 2. , 4. ], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  4., 16., 64.], dtype=float32)&gt;)</span><br></pre>

<p>可以把多个张量<em>连结</em>（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。</p>
<pre class="highlight"><span class="line">X = tf.reshape(tf.<span class="built_in">range</span>(<span class="number">12</span>, dtype=tf.float32), (<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">Y = tf.constant([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">tf.concat([X, Y], axis=<span class="number">0</span>), tf.concat([X, Y], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">6</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]], dtype=float32)&gt;)</span><br></pre>

<h4 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h4><p>广播机制工作方式如下：</p>
<ol>
<li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li>
<li>对生成的数组执行按元素操作。</li>
</ol>
<pre class="highlight"><span class="line">a = tf.reshape(tf.<span class="built_in">range</span>(<span class="number">3</span>), (<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = tf.reshape(tf.<span class="built_in">range</span>(<span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">1</span>), dtype=int32, numpy=</span><br><span class="line"> array([[<span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>]], dtype=int32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">2</span>), dtype=int32, numpy=array([[<span class="number">0</span>, <span class="number">1</span>]], dtype=int32)&gt;)</span><br><span class="line"></span><br><span class="line">a + b</span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">2</span>), dtype=int32, numpy=</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>]], dtype=int32)&gt;</span><br></pre>

<h4 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h4><p>张量中的元素可以通过索引访问。</p>
<pre class="highlight"><span class="line">X[-<span class="number">1</span>], X[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">4</span>,), dtype=float32, numpy=array([ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]], dtype=float32)&gt;)</span><br></pre>

<p>TensorFlow中的<code>Tensors</code>是不可变的，也不能被赋值。 TensorFlow中的<code>Variables</code>是支持赋值的可变容器。</p>
<pre class="highlight"><span class="line">X_var = tf.Variable(X)</span><br><span class="line">X_var[<span class="number">1</span>, <span class="number">2</span>].assign(<span class="number">9</span>)</span><br><span class="line">X_var</span><br><span class="line"></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=(<span class="number">3</span>, <span class="number">4</span>) dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">9.</span>,  <span class="number">7.</span>],</span><br><span class="line">       [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]], dtype=float32)&gt;</span><br></pre>

<h3 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h3><p><code>pandas</code>预处理原始数据，并将原始数据转换为张量格式</p>
<pre class="highlight"><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">   NumRooms Alley   Price</span><br><span class="line"><span class="number">0</span>       NaN  Pave  <span class="number">127500</span></span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>   NaN  <span class="number">106000</span></span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>   NaN  <span class="number">178100</span></span><br><span class="line"><span class="number">3</span>       NaN   NaN  <span class="number">140000</span></span><br></pre>

<p>通过位置索引<code>iloc</code>，我们将<code>data</code>分成<code>inputs</code>和<code>outputs</code>， 其中前者为<code>data</code>的前两列，而后者为<code>data</code>的最后一列。 对于<code>inputs</code>中缺少的数值，我们用同一列的均值替换“NaN”项。</p>
<pre class="highlight"><span class="line">inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"></span><br><span class="line">   NumRooms Alley</span><br><span class="line"><span class="number">0</span>       <span class="number">3.0</span>  Pave</span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>   NaN</span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>   NaN</span><br><span class="line"><span class="number">3</span>       <span class="number">3.0</span>   NaN</span><br></pre>

<p>[<strong>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别。</strong>] 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， <code>pandas</code>可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。</p>
<pre class="highlight"><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"></span><br><span class="line">  NumRooms  Alley_Pave  Alley_nan</span><br><span class="line"><span class="number">0</span>       <span class="number">3.0</span>           <span class="number">1</span>          <span class="number">0</span></span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br><span class="line"><span class="number">3</span>       <span class="number">3.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br></pre>

<h3 id="2-3-线性代数"><a href="#2-3-线性代数" class="headerlink" title="2.3 线性代数"></a>2.3 线性代数</h3><h4 id="2-3-1-标量"><a href="#2-3-1-标量" class="headerlink" title="2.3.1 标量"></a>2.3.1 标量</h4><p>仅包含一个数值被称为<em>标量</em>（scalar）, 如果要将此华氏度值转换为更常用的摄氏度， 则可以计算表达式 $c &#x3D; \frac{5}{9}(f-32)$<br>$\mathbb{R}$表示所有实数标量的空间，表达式 $x \in \mathbb{R}$ 表示x是一个实值标量的正式形式</p>
<h4 id="2-3-2-向量"><a href="#2-3-2-向量" class="headerlink" title="2.3.2 向量"></a>2.3.2 向量</h4><p>向量（vector）可以被视为标量值组成的列表。一个向量X由n个实值标量组成，可以将其表示为 $x \in \mathbb{R}^n$ 向量的长度通常称为向量的维度。<br>形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）</p>
<h4 id="2-3-3-矩阵"><a href="#2-3-3-矩阵" class="headerlink" title="2.3.3 矩阵"></a>2.3.3 矩阵</h4><p>正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶，数学表示法使用 $A \in \mathbb{R}^{m \times n}$ 来表示矩阵A。当我们交换矩阵的行和列时，结果称为矩阵的转置，通常用$a^{\top}$表示矩阵的转置</p>
<h4 id="2-3-4-张量"><a href="#2-3-4-张量" class="headerlink" title="2.3.4 张量"></a>2.3.4 张量</h4><p>张量是描述具有任意数量轴的n维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。</p>
<h4 id="2-3-5-张量算法的基本性质"><a href="#2-3-5-张量算法的基本性质" class="headerlink" title="2.3.5 张量算法的基本性质"></a>2.3.5 张量算法的基本性质</h4><p>给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）数学符号 $\bigodot$</p>
<pre class="highlight"><span class="line">A * B</span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">5</span>, <span class="number">4</span>), dtype=float32, numpy=</span><br><span class="line">array([[  <span class="number">0.</span>,   <span class="number">1.</span>,   <span class="number">4.</span>,   <span class="number">9.</span>],</span><br><span class="line">       [ <span class="number">16.</span>,  <span class="number">25.</span>,  <span class="number">36.</span>,  <span class="number">49.</span>],</span><br><span class="line">       [ <span class="number">64.</span>,  <span class="number">81.</span>, <span class="number">100.</span>, <span class="number">121.</span>],</span><br><span class="line">       [<span class="number">144.</span>, <span class="number">169.</span>, <span class="number">196.</span>, <span class="number">225.</span>],</span><br><span class="line">       [<span class="number">256.</span>, <span class="number">289.</span>, <span class="number">324.</span>, <span class="number">361.</span>]], dtype=float32)&gt;</span><br></pre>

<p>将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。</p>
<h4 id="2-3-6-降维"><a href="#2-3-6-降维" class="headerlink" title="2.3.6 降维"></a>2.3.6 降维</h4><p>我们可以对任意张量进行的一个有用的操作是计算其元素的和。 数学表示法使用 $\sum$ 符号表示求和。为了表示长度为d的向量中元素的总和，可以记为 $\sum^d_{i&#x3D;1}x_i$ 代码里的求和函数</p>
<pre class="highlight"><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">4</span>, dtype=tf.float32)</span><br><span class="line">x, tf.reduce_sum(x)</span><br><span class="line"></span><br><span class="line">(&lt;tf.Tensor: shape=(<span class="number">4</span>,), dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">6.0</span>&gt;)</span><br></pre>

<p>我们可以表示任意形状张量的元素和。例如矩阵A中元素的和可以记为 $\sum^m_{i&#x3D;1}\sum^n_{j&#x3D;1}a_{ij}$</p>
<p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。我们还可以指定张量沿哪一个轴来通过求和降低维度。以矩阵为例，为了通过求和所有行的元素来降维（轴0 纵轴），可以在调用函数时指定<code>axis=0</code>。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。</p>
<pre class="highlight"><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line">(array([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]), (<span class="number">4</span>,))</span><br></pre>

<p>与求和相关的量是<em>平均值</em>（mean或average）</p>
<p>有时在调用函数来计算总和或均值时保持轴数不变会很有用</p>
<pre class="highlight"><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">sum_A</span><br><span class="line"></span><br><span class="line">array([[ <span class="number">6.</span>],</span><br><span class="line">       [<span class="number">22.</span>],</span><br><span class="line">       [<span class="number">38.</span>],</span><br><span class="line">       [<span class="number">54.</span>],</span><br><span class="line">       [<span class="number">70.</span>]])</span><br></pre>

<h4 id="2-3-7-点积"><a href="#2-3-7-点积" class="headerlink" title="2.3.7 点积"></a>2.3.7 点积</h4><p>给定两个向量x,y，他们的点积是相同位置的按元素乘积的和 $x^{\Tau}y&#x3D;\sum^d_{i&#x3D;1}x_iy_i$</p>
<pre class="highlight"><span class="line">y = np.ones(<span class="number">4</span>)</span><br><span class="line">x, y, np.dot(x, y)</span><br><span class="line"></span><br><span class="line">(array([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]), array([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]), array(<span class="number">6.</span>))</span><br></pre>

<h4 id="2-3-8-矩阵-向量积"><a href="#2-3-8-矩阵-向量积" class="headerlink" title="2.3.8 矩阵-向量积"></a>2.3.8 矩阵-向量积</h4><p>定义矩阵 $A \in \mathbb{R}^{m \times n}$ 和向量 $x \in \mathbb{R}^n$ 我们让矩阵A用它的行向量表示：<br>$$<br>A &#x3D; \begin{bmatrix}<br>  a^{\Tau}_1\<br>  a^{\Tau}_2\<br>  \vdots\<br>  a^{\Tau}_m<br>\end{bmatrix}<br>$$<br>其中每个 $a^{\Tau}_i \in \mathbb{R}^n$ 都是行向量，表示矩阵的第i行。矩阵向量积Ax是一个长度为m的列向量，其第i个元素是点积 $a^{\Tau}_ix$<br>$$<br>Ax &#x3D; \begin{bmatrix}<br>  a^{\Tau}_1\<br>  a^{\Tau}_2\<br>  \vdots\<br>  a^{\Tau}_m<br>\end{bmatrix}x&#x3D;\begin{bmatrix}<br>  a^{\Tau}_1x\<br>  a^{\Tau}_2x\<br>  \vdots\<br>  a^{\Tau}_mx<br>\end{bmatrix}<br>$$<br>我们也可以使用矩阵-向量积来描述在给定前一层的值时， 求解神经网络每一层所需的复杂计算</p>
<pre class="highlight"><span class="line">A.shape, x.shape, tf.linalg.matvec(A, x)</span><br><span class="line"></span><br><span class="line">(TensorShape([<span class="number">5</span>, <span class="number">4</span>]),</span><br><span class="line"> TensorShape([<span class="number">4</span>]),</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">5</span>,), dtype=float32, numpy=array([ <span class="number">14.</span>,  <span class="number">38.</span>,  <span class="number">62.</span>,  <span class="number">86.</span>, <span class="number">110.</span>], dtype=float32)&gt;)</span><br></pre>

<h4 id="2-3-9-矩阵-矩阵乘法"><a href="#2-3-9-矩阵-矩阵乘法" class="headerlink" title="2.3.9 矩阵-矩阵乘法"></a>2.3.9 矩阵-矩阵乘法</h4><p>假设有两个矩阵 $A \in \mathbb{R}^{n \times k}$ 和 $B \in \mathbb{R}^{k \times m}$ 用行向量 $a^{\Tau}_i \in \mathbb{R}^k$ 表示矩阵A的第i行，并让列向量 $b_j \in \mathbb{R}^k$ 作为矩阵B的第j列。要生成矩阵积 $C &#x3D; B \times A$ ，最简单的方法是考虑A的行向量和B的列向量:<br>$$<br>C&#x3D;A \times B&#x3D;\begin{bmatrix}<br>  a^{\Tau}_1\<br>  a^{\Tau}_2\<br>  \vdots\<br>  a^{\Tau}_m<br>\end{bmatrix}\begin{bmatrix}b_1&amp;b_2&amp;\cdots&amp; b_m\end{bmatrix}&#x3D;\begin{bmatrix}<br>  a^{\Tau}_1b_1&amp;a^{\Tau}_1b_2&amp;\cdots&amp;a^{\Tau}_1b_m\<br>  a^{\Tau}_2b_1&amp;a^{\Tau}_2b_2&amp;\cdots&amp;a^{\Tau}_2b_m\<br>  {\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\<br>  a^{\Tau}_nb_1&amp;a^{\Tau}_nb_2&amp;\cdots&amp;a^{\Tau}_nb_m<br>\end{bmatrix}<br>$$</p>
<h4 id="2-3-10-范数"><a href="#2-3-10-范数" class="headerlink" title="2.3.10 范数"></a>2.3.10 范数</h4><p>向量的<em>范数</em>是表示一个向量有多大。</p>
<p>在线性代数中，向量范数是将向量映射到标量的函数 $f$ 。 给定任意向量x，向量范数要满足一些属性。 第一个性质是：如果我们按常数因子 $\alpha$ 缩放向量的所有元素， 其范数也会按相同常数因子的<em>绝对值</em>缩放：<br>$$<br>f(\alpha x)&#x3D;|\alpha|f(x)<br>$$<br>第二个性质是熟悉的三角不等式:<br>$$<br>f(x+y) \leq f(x) + f(y)<br>$$<br>第三个性质简单地说范数必须是非负的:<br>$$<br>f(x) \geq 0<br>$$<br>这是有道理的。因为在大多数情况下，任何东西的最小的<em>大小</em>是0。 最后一个性质要求范数最小为0，当且仅当向量全由0组成。 $\forall$ 表示任何一个<br>$$<br>\forall i,\begin{bmatrix}<br>  x<br>\end{bmatrix}_i&#x3D;0 \Leftrightarrow f(x)&#x3D;0<br>$$<br>欧几里得距离是一个 $L_2$ 范数： 假设n维向量x中的元素是 $x_1,\cdots x_n$ ，其 $L_2$ <em>范数</em>是向量元素平方和的平方根：<br>$$<br>||x||<em>2&#x3D;\sqrt{\sum^n</em>{i&#x3D;1}x^2_i}<br>$$<br>计算方式为：</p>
<pre class="highlight"><span class="line">u = tf.constant([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">tf.norm(u)</span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">5.0</span>&gt;</span><br></pre>

<p>矩阵 $X \in \mathbb{R}^{m \times n}$ 的 Frobenius范数是矩阵元素平方和和平方根：<br>$$<br>||x||<em>F&#x3D;\sqrt{\sum^m</em>{i&#x3D;1}\sum^n_{j&#x3D;1}x^2_{ij}}<br>$$</p>
<h4 id="2-4-微积分"><a href="#2-4-微积分" class="headerlink" title="2.4 微积分"></a>2.4 微积分</h4><h5 id="2-4-1-导数和微分"><a href="#2-4-1-导数和微分" class="headerlink" title="2.4.1 导数和微分"></a>2.4.1 导数和微分</h5><p>在深度学习中，我们通常选择对于模型参数可微的损失函数。 简而言之，对于每个参数， 如果我们把这个参数<em>增加</em>或<em>减少</em>一个无穷小的量，可以知道损失会以多快的速度增加或减少，</p>
<p>假设我们有一个函数$f$ ：$\mathbb{R}\rightarrow\mathbb{R}$，其输入和输出都是标量。如果$f$的导数存在，这个极限被定义为<br>$$<br>f^{\prime}(x)&#x3D;\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}<br>$$<br>如果$f^{\prime}(a)$存在，则称f在a处是可微的，将$f^{\prime}(x)$解释为f(x)相对于x的瞬时（instantaneous）变化率</p>
<p>熟悉一下导数的几个等价符号。给定y&#x3D;f(x)，其中x和y分别是函数$f$的自变量和因变量。以下表达式是等价的：<br>$$<br>f^{\prime}(x)&#x3D;y^{\prime}&#x3D;\frac{\mathrm{d}y}{\mathrm{d}x}&#x3D;\frac{\mathrm{d}f}{\mathrm{d}x}&#x3D;\frac{\mathrm{d}}{\mathrm{d}x}f(x)&#x3D;Df(x)&#x3D;D_xf(x)<br>$$<br>其中符号$\frac{\mathrm{d}}{\mathrm{d}x}$和D是微分运算符，表示微分操作。我们可以使用以下规则来对常见函数求微分：</p>
<ul>
<li>$DC&#x3D;0$（C是一个常数）</li>
<li>$Dx^n&#x3D;nx^n-1$（power rule,n是任意实数）</li>
<li>$De^x&#x3D;e^e$</li>
<li>$D\ln(x)&#x3D;1&#x2F;x$</li>
</ul>
<p>为了微分一个由常见函数组成的函数，下面一些法则方便使用。假设$f$和$g$都是可微的，C是一个常数，则常数相乘法则：<br>$$<br>\frac{\mathrm{d}}{\mathrm{d}x}[Cf(x)]&#x3D;C\frac{\mathrm{d}}{\mathrm{d}x}f(x)<br>$$<br>加法法则：<br>$$<br>\frac{\mathrm{d}}{\mathrm{d}x}[f(x)+g(x)]&#x3D;\frac{\mathrm{d}}{\mathrm{d}x}f(x)+\frac{\mathrm{d}}{\mathrm{d}x}g(x)<br>$$<br>乘法法则：<br>$$<br>\frac{\mathrm{d}}{\mathrm{d}x}[f(x)g(x)]&#x3D;f(x)\frac{\mathrm{d}}{\mathrm{d}x}[g(x)]+g(x)\frac{\mathrm{d}}{\mathrm{d}x}[f(x)]<br>$$<br>除法法则：<br>$$<br>\frac{\mathrm{d}}{\mathrm{d}x}[\frac{f(x)}{g(x)}]&#x3D;\frac{g(x)\frac{\mathrm{d}}{\mathrm{d}x}[f(x)]-f(x)\frac{\mathrm{d}}{\mathrm{d}x}[g(x)]}{[g(x)]^2}<br>$$</p>
<h5 id="2-4-2-偏导数"><a href="#2-4-2-偏导数" class="headerlink" title="2.4.2 偏导数"></a>2.4.2 偏导数</h5><p>深度学习中，函数通常依赖许多变量。因此我们需要将微分思想推广到多元函数</p>
<p>设$y&#x3D;f(x_1,x_2,\ldots,x_n)$是一个具有n个变量的函数。y关于第i个参数$x_i$的偏导数为（partial derivative）：<br>$$<br>\frac{\partial{y}}{\partial{x_i}}&#x3D;\lim_{h\rightarrow0}\frac{f(x_1,\ldots,x_{i-1},x_i+h,x_{i+1},\ldots,x_n)}{h}<br>$$<br>对于偏导数的表示，以下是等价的：<br>$$<br>\frac{\partial{y}}{\partial{x_i}}&#x3D;\frac{\partial{f}}{\partial{x_i}}&#x3D;f_{xi}&#x3D;f_i&#x3D;D_if&#x3D;D_{xi}f<br>$$</p>
<h5 id="2-4-3-梯度"><a href="#2-4-3-梯度" class="headerlink" title="2.4.3 梯度"></a>2.4.3 梯度</h5><p>我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的梯度向量，设函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$的输入是一个n维向量$X&#x3D;[x_1,x_2,\ldots,x_n]\top$,并且输出是一个标量。函数f(X)相对于X的梯度是一个包含n个偏导数的向量：<br>$$<br>\nabla{x} f(X)&#x3D;[\frac{\partial{f(x)}}{\partial{x_1}},\frac{\partial{f(x)}}{\partial{x_2}},\ldots,\frac{\partial{f(x)}}{\partial{x_n}}]\top<br>$$<br>假设X为n维向量，在微分多元函数时经常使用以下规则：</p>
<ul>
<li>对于所有$A\in\mathbb{R}^{m \times n}$,都有$\nabla{x} AX&#x3D;A^\top$</li>
<li>对于所有的$A\in\mathbb{R}^{m \times n}$,都有$\nabla{x} X^{\top}A&#x3D;A$</li>
<li>对于所有的$A\in\mathbb{R}^{m \times n}$,都有$\nabla{x} X^{\top}AX&#x3D;(A+A^{\top})X$</li>
<li>$\nabla{x} \left|x\right|^2&#x3D;\nabla{x}X^{\top}X&#x3D;2X$</li>
</ul>
<h5 id="2-4-4-链式法则"><a href="#2-4-4-链式法则" class="headerlink" title="2.4.4 链式法则"></a>2.4.4 链式法则</h5><p>多元函数是复合的，链式法则可以被用来微分复合函数。</p>
<p>先考虑单变量函数。假设函数y&#x3D;f(u)和u&#x3D;g(x)都是可微的，根据链式法则：<br>$$<br>\frac{\mathrm{d}y}{\mathrm{d}x}&#x3D;\frac{\mathrm{d}y}{\mathrm{d}u}\frac{\mathrm{d}u}{\mathrm{d}x}<br>$$<br>现在考虑一个更一般的场景，即函数具有任意数量的变量的情况。 假设可微分函数$y$有变量$u_1,u_2,\ldots,u_m$，其中每个可微分函数$u_i$都有变量$x_1,x_2,\ldots,x_n$。 注意，$y$是$u_1,u_2,\ldots,u_m$的函数。 对于任意$i&#x3D;1,2,\ldots,n$，链式法则给出：<br>$$<br>\frac{\partial{y}}{\partial{x_i}}&#x3D;\frac{\partial{y}}{\partial{u_i}}\frac{\partial{u_1}}{\partial{x_i}}+\frac{\partial{y}}{\partial{u_2}}\frac{\partial{u_2}}{\partial{x_i}}+\ldots+\frac{\partial{y}}{\partial{u_m}}\frac{\partial{u_m}}{\partial{x_i}}<br>$$</p>

  </div>
  <div>
    
      <div 
        class="post-note note-warning copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/java4u/about">
            Zhao Xin
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="https://zxtc123.github.io/java4u/2023/04/25/AI/">
            https://zxtc123.github.io/java4u/2023/04/25/AI/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
    <div class="nav-item-prev">
      <a 
        href="/java4u/2023/05/16/RUST_tutorial1/" 
        class="nav-link">
        <i class="iconfont icon-left nav-prev-icon"></i>
        <div>
          <div class="nav-label">上一篇</div>
          
            <div class="nav-title">RUST入门 </div>
          
        </div>
      </a>
    </div>
  
  
    <div class="nav-item-next">
      <a 
        href="/java4u/2023/02/01/linux_command/" 
        class="nav-link">
        <div>
          <div class="nav-label">下一篇</div>
          
            <div class="nav-title">linux深入了解 </div>
          
        </div>
        <i class="iconfont icon-right nav-next-icon"></i>
      </a>
    </div>
  
</div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%85%A5%E9%97%A8"><span class="toc-text">人工智能入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-text">机器学习基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E5%BC%95%E8%A8%80"><span class="toc-text">一. 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%B8%B8%E7%94%9F%E6%B4%BB%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-text">日常生活机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">1. 监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E5%9B%9E%E5%BD%92"><span class="toc-text">1.1 回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%88%86%E7%B1%BB"><span class="toc-text">1.2 分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E6%A0%87%E8%AE%B0%E9%97%AE%E9%A2%98"><span class="toc-text">1.3 标记问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-%E6%90%9C%E7%B4%A2"><span class="toc-text">1.4 搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="toc-text">1.5 推荐系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.6 序列学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">2. 无监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%B8%8E%E7%8E%AF%E5%A2%83%E4%BA%92%E5%8A%A8"><span class="toc-text">3. 与环境互动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text">4. 强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-text">二. 预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-text">2.1 数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-text">运算符</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="toc-text">索引和切片</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">2.2 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-text">2.3 线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-%E6%A0%87%E9%87%8F"><span class="toc-text">2.3.1 标量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-%E5%90%91%E9%87%8F"><span class="toc-text">2.3.2 向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-%E7%9F%A9%E9%98%B5"><span class="toc-text">2.3.3 矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-%E5%BC%A0%E9%87%8F"><span class="toc-text">2.3.4 张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-5-%E5%BC%A0%E9%87%8F%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%A7%E8%B4%A8"><span class="toc-text">2.3.5 张量算法的基本性质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-6-%E9%99%8D%E7%BB%B4"><span class="toc-text">2.3.6 降维</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-7-%E7%82%B9%E7%A7%AF"><span class="toc-text">2.3.7 点积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-8-%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-text">2.3.8 矩阵-向量积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-9-%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-text">2.3.9 矩阵-矩阵乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-10-%E8%8C%83%E6%95%B0"><span class="toc-text">2.3.10 范数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-text">2.4 微积分</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-1-%E5%AF%BC%E6%95%B0%E5%92%8C%E5%BE%AE%E5%88%86"><span class="toc-text">2.4.1 导数和微分</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-2-%E5%81%8F%E5%AF%BC%E6%95%B0"><span class="toc-text">2.4.2 偏导数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-3-%E6%A2%AF%E5%BA%A6"><span class="toc-text">2.4.3 梯度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-4-%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-text">2.4.4 链式法则</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
</div></main>
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="/java4u/img/1.e3a1f41f.gif" 
    class="author-img"
    width="88"
    height="88"
    alt="author avatar">

<p class="author-name">Zhao Xin</p>
<p class="author-description">5年工作经验，不秃头的java萌新。</p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/java4u/archives">
    <span>112</span>
    <span>文章</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/java4u/categories">
    <span>24</span>
    <span>分类</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/java4u/tags">
    <span>31</span>
    <span>标签</span>
  </a>
</div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%85%A5%E9%97%A8"><span class="toc-text">人工智能入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-text">机器学习基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E5%BC%95%E8%A8%80"><span class="toc-text">一. 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%B8%B8%E7%94%9F%E6%B4%BB%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-text">日常生活机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">1. 监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E5%9B%9E%E5%BD%92"><span class="toc-text">1.1 回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%88%86%E7%B1%BB"><span class="toc-text">1.2 分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E6%A0%87%E8%AE%B0%E9%97%AE%E9%A2%98"><span class="toc-text">1.3 标记问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-%E6%90%9C%E7%B4%A2"><span class="toc-text">1.4 搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="toc-text">1.5 推荐系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.6 序列学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">2. 无监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%B8%8E%E7%8E%AF%E5%A2%83%E4%BA%92%E5%8A%A8"><span class="toc-text">3. 与环境互动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text">4. 强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-text">二. 预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-text">2.1 数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-text">运算符</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="toc-text">索引和切片</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">2.2 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-text">2.3 线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-%E6%A0%87%E9%87%8F"><span class="toc-text">2.3.1 标量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-%E5%90%91%E9%87%8F"><span class="toc-text">2.3.2 向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-%E7%9F%A9%E9%98%B5"><span class="toc-text">2.3.3 矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-%E5%BC%A0%E9%87%8F"><span class="toc-text">2.3.4 张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-5-%E5%BC%A0%E9%87%8F%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%A7%E8%B4%A8"><span class="toc-text">2.3.5 张量算法的基本性质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-6-%E9%99%8D%E7%BB%B4"><span class="toc-text">2.3.6 降维</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-7-%E7%82%B9%E7%A7%AF"><span class="toc-text">2.3.7 点积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-8-%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-text">2.3.8 矩阵-向量积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-9-%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-text">2.3.9 矩阵-矩阵乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-10-%E8%8C%83%E6%95%B0"><span class="toc-text">2.3.10 范数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-text">2.4 微积分</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-1-%E5%AF%BC%E6%95%B0%E5%92%8C%E5%BE%AE%E5%88%86"><span class="toc-text">2.4.1 导数和微分</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-2-%E5%81%8F%E5%AF%BC%E6%95%B0"><span class="toc-text">2.4.2 偏导数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-3-%E6%A2%AF%E5%BA%A6"><span class="toc-text">2.4.3 梯度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-4-%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-text">2.4.4 链式法则</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>分类
  </div>
  <div class="categories-list">
    
      <a href="/java4u/categories/%E7%BD%91%E7%BB%9C/">
        <div class="categories-list-item">
          网络
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/webservice/">
        <div class="categories-list-item">
          webservice
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E5%8A%A0%E5%AF%86%E8%A7%A3%E7%A0%81/">
        <div class="categories-list-item">
          加密解码
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">
        <div class="categories-list-item">
          数据结构与算法
          <span class="categories-list-item-badge">55</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E7%BA%BF%E7%A8%8B%E6%B1%A0/">
        <div class="categories-list-item">
          线程池
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E9%9D%A2%E8%AF%95/">
        <div class="categories-list-item">
          面试
          <span class="categories-list-item-badge">7</span>
        </div>
      </a>
    
      <a href="/java4u/categories/security/">
        <div class="categories-list-item">
          security
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/JVM/">
        <div class="categories-list-item">
          JVM
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E5%88%86%E5%B8%83%E5%BC%8F/">
        <div class="categories-list-item">
          分布式
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E6%A1%86%E6%9E%B6/">
        <div class="categories-list-item">
          框架
          <span class="categories-list-item-badge">5</span>
        </div>
      </a>
    
      <a href="/java4u/categories/SQL/">
        <div class="categories-list-item">
          SQL
          <span class="categories-list-item-badge">20</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E8%BF%9B%E7%A8%8B/">
        <div class="categories-list-item">
          进程
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/">
        <div class="categories-list-item">
          中间件
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93/">
        <div class="categories-list-item">
          文档总结
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BF%9D%E6%8A%A4/">
        <div class="categories-list-item">
          微服务保护
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E7%BC%93%E5%AD%98/">
        <div class="categories-list-item">
          缓存
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E4%BA%8B%E5%8A%A1/">
        <div class="categories-list-item">
          事务
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E5%AE%B9%E5%99%A8/">
        <div class="categories-list-item">
          容器
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E8%A7%84%E8%8C%83/">
        <div class="categories-list-item">
          规范
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E8%AF%AD%E8%A8%80/">
        <div class="categories-list-item">
          语言
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">
        <div class="categories-list-item">
          编程语言
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/java4u/categories/linux/">
        <div class="categories-list-item">
          linux
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2/">
        <div class="categories-list-item">
          分布式搜索
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/java4u/categories/AI/">
        <div class="categories-list-item">
          AI
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>热门标签
  </div>
  <div class="tags-list">
    
      <a 
        href="/java4u/tags/leetcode/" 
        title="leetcode">
        <div class="tags-list-item">leetcode</div>
      </a>
    
      <a 
        href="/java4u/tags/%E5%AD%A6%E4%B9%A0/" 
        title="学习">
        <div class="tags-list-item">学习</div>
      </a>
    
      <a 
        href="/java4u/tags/%E6%95%B0%E7%BB%84/" 
        title="数组">
        <div class="tags-list-item">数组</div>
      </a>
    
      <a 
        href="/java4u/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" 
        title="动态规划">
        <div class="tags-list-item">动态规划</div>
      </a>
    
      <a 
        href="/java4u/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" 
        title="二叉树">
        <div class="tags-list-item">二叉树</div>
      </a>
    
      <a 
        href="/java4u/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" 
        title="字符串">
        <div class="tags-list-item">字符串</div>
      </a>
    
      <a 
        href="/java4u/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/" 
        title="位运算">
        <div class="tags-list-item">位运算</div>
      </a>
    
      <a 
        href="/java4u/tags/%E4%BA%8C%E5%88%86%E6%B3%95/" 
        title="二分法">
        <div class="tags-list-item">二分法</div>
      </a>
    
      <a 
        href="/java4u/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" 
        title="滑动窗口">
        <div class="tags-list-item">滑动窗口</div>
      </a>
    
      <a 
        href="/java4u/tags/%E9%93%BE%E8%A1%A8/" 
        title="链表">
        <div class="tags-list-item">链表</div>
      </a>
    
      <a 
        href="/java4u/tags/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/" 
        title="参考资料">
        <div class="tags-list-item">参考资料</div>
      </a>
    
      <a 
        href="/java4u/tags/django/" 
        title="django">
        <div class="tags-list-item">django</div>
      </a>
    
      <a 
        href="/java4u/tags/%E6%BA%90%E7%A0%81/" 
        title="源码">
        <div class="tags-list-item">源码</div>
      </a>
    
      <a 
        href="/java4u/tags/kubernetes/" 
        title="kubernetes">
        <div class="tags-list-item">kubernetes</div>
      </a>
    
      <a 
        href="/java4u/tags/spring/" 
        title="spring">
        <div class="tags-list-item">spring</div>
      </a>
    
      <a 
        href="/java4u/tags/python/" 
        title="python">
        <div class="tags-list-item">python</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%85%A5%E9%97%A8"><span class="toc-text">人工智能入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-text">机器学习基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E5%BC%95%E8%A8%80"><span class="toc-text">一. 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%B8%B8%E7%94%9F%E6%B4%BB%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-text">日常生活机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">1. 监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E5%9B%9E%E5%BD%92"><span class="toc-text">1.1 回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%88%86%E7%B1%BB"><span class="toc-text">1.2 分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E6%A0%87%E8%AE%B0%E9%97%AE%E9%A2%98"><span class="toc-text">1.3 标记问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-%E6%90%9C%E7%B4%A2"><span class="toc-text">1.4 搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="toc-text">1.5 推荐系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.6 序列学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">2. 无监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%B8%8E%E7%8E%AF%E5%A2%83%E4%BA%92%E5%8A%A8"><span class="toc-text">3. 与环境互动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text">4. 强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-text">二. 预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-text">2.1 数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-text">运算符</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="toc-text">索引和切片</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">2.2 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-text">2.3 线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-%E6%A0%87%E9%87%8F"><span class="toc-text">2.3.1 标量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-%E5%90%91%E9%87%8F"><span class="toc-text">2.3.2 向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-%E7%9F%A9%E9%98%B5"><span class="toc-text">2.3.3 矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-%E5%BC%A0%E9%87%8F"><span class="toc-text">2.3.4 张量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-5-%E5%BC%A0%E9%87%8F%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%A7%E8%B4%A8"><span class="toc-text">2.3.5 张量算法的基本性质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-6-%E9%99%8D%E7%BB%B4"><span class="toc-text">2.3.6 降维</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-7-%E7%82%B9%E7%A7%AF"><span class="toc-text">2.3.7 点积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-8-%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-text">2.3.8 矩阵-向量积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-9-%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-text">2.3.9 矩阵-矩阵乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-10-%E8%8C%83%E6%95%B0"><span class="toc-text">2.3.10 范数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-text">2.4 微积分</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-1-%E5%AF%BC%E6%95%B0%E5%92%8C%E5%BE%AE%E5%88%86"><span class="toc-text">2.4.1 导数和微分</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-2-%E5%81%8F%E5%AF%BC%E6%95%B0"><span class="toc-text">2.4.2 偏导数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-3-%E6%A2%AF%E5%BA%A6"><span class="toc-text">2.4.3 梯度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-4-%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-text">2.4.4 链式法则</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>最近文章
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-03-10</div>
        <a href="/java4u/2024/03/10/MQ/"><div class="recent-posts-item-content">MQ</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-03-10</div>
        <a href="/java4u/2024/03/10/2.jvm/"><div class="recent-posts-item-content">JVM</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2023-06-04</div>
        <a href="/java4u/2023/06/04/RUST_tutorial2/"><div class="recent-posts-item-content">RUST入门（二）</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2023-05-16</div>
        <a href="/java4u/2023/05/16/RUST_tutorial1/"><div class="recent-posts-item-content">RUST入门</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2020
          
          
                - 
                2024
          
        </span>
        &nbsp;
        <a 
          href="/java4u/" 
          class="footer-link">
          Snail
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          Theme -
          <a 
            href="https://github.com/theme-kaze" 
            class="footer-link" 
            target="_blank"
            rel="nofollow noopener noreferrer">
            &nbsp;Kaze
          </a>
        
      </div>
    
    
    
    
</footer>
 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton"
  aria-label="menu button"
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
      
 
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/java4u/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/java4u/js/main.js"></script> 
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/java4u/js/lib/lozad.min.js', addLazyload)
      </script>
     
    
    
      <script>
        setTimeout(() => {localSearch("search.json")}, 0)
      </script>
    
  </body>
</html>
